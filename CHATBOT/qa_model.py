# -*- coding: utf-8 -*-
"""qa_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10HZQsncoArGMEVnDLr8JlN3PyebmbQ-K
"""

import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer, util

def load_models():
    embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
    qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

    gen_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
    gen_tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
    generator = pipeline("text2text-generation", model=gen_model, tokenizer=gen_tokenizer)

    return embedder, qa_pipeline, generator

def elaborate_answer(question, short_answer, context, generator):
    prompt = f"""Question: {question}
Short Answer: {short_answer}
Context: {context}
Elaborate on this answer using the context. Make sure to use the correct grammar."""
    output = generator(prompt, max_new_tokens=256, do_sample=False)
    return output[0]['generated_text']

def answer_question(question, df, chunk_embeddings, model, qa_pipeline, generator,
                    top_k=5, threshold=0.35, confidence_threshold=0.9):
    question_embedding = model.encode(question)
    similarities = util.cos_sim(question_embedding, chunk_embeddings)[0]
    top_indices = similarities.argsort(descending=True)[:top_k]

    for idx in top_indices:
        idx_int = idx.item()
        score = similarities[idx_int].item()

        if score >= threshold:
            context = df.iloc[idx_int]['text']
            extractive_output = qa_pipeline(question=question, context=context)
            short_answer = extractive_output['answer']
            confidence = extractive_output.get('score', 0.0)

            if confidence < confidence_threshold:
                return elaborate_answer(question, short_answer, context, generator), context
            else:
                return short_answer, context

    return "Sorry, I couldn't find an answer.", None