# -*- coding: utf-8 -*-
"""scrape.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mF4GUGQ6aJzWPhQcoZUDOr4_nTf07CZ3
"""

import requests
from bs4 import BeautifulSoup
import os
import re
import pandas as pd

def clean_text(text):
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def is_valid_text(text, min_words=8):
    words = text.strip().split()
    return len(words) >= min_words and any(punct in text for punct in ',.?!')

def scrape_html(url, label):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        for tag in soup(['script', 'style', 'noscript']):
            tag.decompose()

        content_blocks = []
        block_counter = 1
        for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):
            text = tag.get_text(separator=' ', strip=True)
            cleaned = clean_text(text)
            if cleaned and is_valid_text(cleaned):
                content_blocks.append([label, f"{label}_block_{block_counter}", cleaned])
                block_counter += 1

        return content_blocks

    except requests.RequestException as e:
        print(f"[ERROR] Could not scrape {url}: {e}")
        return []

def save_to_csv(data, filename="scraped_data.csv"):
    df = pd.DataFrame(data, columns=["source", "section", "text"])
    os.makedirs("scraped_output", exist_ok=True)
    df.to_csv(f"scraped_output/{filename}", index=False, encoding='utf-8')
    print(f"Saved {filename} with {len(df)} rows.")